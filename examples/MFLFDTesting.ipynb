{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import trecs\n",
    "from trecs.models import ImplicitMF, ImplicitMFLFD\n",
    "from trecs.random import Generator\n",
    "from trecs.metrics import MSEMeasurement, AverageFeatureScoreRange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  6.92it/s]\n",
      "/Users/amywinecoff/Documents/CITP/Research/Github/t-recs/trecs/models/mf.py:262: UserWarning: train_between_steps is set to True. Note that, at each step, this overwrites the MF model with a model fit only to the latest interaction. To avoid this behavior, set train_between_steps to False.\n",
      "  \"train_between_steps is set to True. Note that, at each step, this \"\n",
      "100%|██████████| 20/20 [00:15<00:00,  1.31it/s]\n"
     ]
    }
   ],
   "source": [
    "mf = ImplicitMF(num_users=2000, num_items=500, num_latent_factors=20, num_items_per_iter=10)\n",
    "mf.add_metrics(MSEMeasurement())\n",
    "mf.add_metrics(AverageFeatureScoreRange())\n",
    "mf.startup_and_train(20)\n",
    "mf.run(timesteps=20, train_between_steps=True, reset_interactions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:02<00:00,  6.95it/s]\n",
      " 50%|█████     | 10/20 [00:46<00:46,  4.64s/it]"
     ]
    }
   ],
   "source": [
    "mflfd = ImplicitMFLFD(num_users=2000, num_items=500, num_latent_factors=20, num_items_per_iter=10)\n",
    "mflfd.add_metrics(MSEMeasurement())\n",
    "mflfd.add_metrics(AverageFeatureScoreRange())\n",
    "mflfd.startup_and_train(20)\n",
    "mflfd.run(timesteps=20, train_between_steps=True, reset_interactions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mflfd_metrics = pd.DataFrame(mflfd.get_measurements())\n",
    "mflfd_afsr= mflfd_metrics['afsr'].to_list()[21:]\n",
    "\n",
    "mf_metrics = pd.DataFrame(mf.get_measurements())\n",
    "mf_afsr= mf_metrics['afsr'].to_list()[21:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "# create a color palette\n",
    "palette = plt.get_cmap('Set1')\n",
    "\n",
    "plt.plot(list(range(len(mflfd_afsr))), mflfd_afsr, marker='', color=palette(0), linewidth=1, alpha=0.9, label='MF-LFD')\n",
    "plt.plot(list(range(len(mf_afsr))), mf_afsr, marker='', color=palette(1), linewidth=1, alpha=0.9, label='MF')\n",
    "\n",
    "# Add legend\n",
    "#plt.legend(loc=2, ncol=2)\n",
    "plt.legend(loc=1, ncol=1)\n",
    "\n",
    "# Add titles\n",
    "plt.title(\"AFSR for MF vs. MF-LFD with Repeated Training\", loc='center', fontsize=16, fontweight=2)\n",
    "plt.xlabel(\"Timestep\")\n",
    "plt.ylabel(\"AFSR\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_n_limit=50\n",
    "# k=10\n",
    "# #self=mflfd\n",
    "\n",
    "# def generate_recs(self, k=1, top_n_limit=None, item_indices=None):\n",
    "    \n",
    "#     if item_indices is not None:\n",
    "#         if item_indices.size < self.num_users:\n",
    "#             raise ValueError(\n",
    "#                     \"At least one user has interacted with all items!\"\n",
    "#                     \"To avoid this problem, you may want to allow repeated items.\"\n",
    "#                 )\n",
    "#         if k > item_indices.shape[1]:\n",
    "#             raise ValueError(\n",
    "#                     f\"There are not enough items left to recommend {k} items to each user.\"\n",
    "#                 )\n",
    "#         if k == 0:\n",
    "#             return np.array([]).reshape((self.num_users, 0)).astype(int)\n",
    "    \n",
    "#     if not top_n_limit:\n",
    "#         top_n_limit = self.items_hat.shape[1]       \n",
    "        \n",
    "#     row = np.repeat(self.users.user_vector, self.item_indices.shape[1])\n",
    "#     row = row.reshape((self.num_users, -1))\n",
    "#     s_filtered = self.predicted_scores[row, self.item_indices]\n",
    "\n",
    "#     negated_scores = -1 * s_filtered  # negate scores so indices go from highest to lowest\n",
    "#     # break ties using a random score component\n",
    "#     scores_tiebreak = np.zeros(\n",
    "#         negated_scores.shape, dtype=[(\"score\", \"f8\"), (\"random\", \"f8\")]\n",
    "#     )\n",
    "#     scores_tiebreak[\"score\"] = negated_scores\n",
    "#     scores_tiebreak[\"random\"] = self.random_state.random(negated_scores.shape)\n",
    "#     top_k = scores_tiebreak.argpartition(top_n_limit - 1, order=[\"score\", \"random\"])[:, :top_n_limit]\n",
    "#     # now we sort within the top k\n",
    "#     row = np.repeat(self.users.user_vector, top_n_limit).reshape((self.num_users, -1))\n",
    "#     # again, indices should go from highest to lowest\n",
    "#     sort_top_k = scores_tiebreak[row, top_k].argsort(order=[\"score\", \"random\"])\n",
    "#     top_k_recs = self.item_indices[row, top_k[row, sort_top_k]]\n",
    "\n",
    "#     #dims are attribute, items, users\n",
    "#     top_k_att = mflfd.items_hat[:, top_k_recs[:]].swapaxes(1,2)\n",
    "    \n",
    "#     rec = []\n",
    "#     for idx, user in enumerate(mflfd.users_hat):\n",
    "\n",
    "#             #make a copy so as not to modify the original array\n",
    "#             user_item_feats = np.array(top_k_att[:,:,idx])\n",
    "\n",
    "#             orig_user_item_feats = np.array(user_item_feats)\n",
    "#             #user_item_feats_idx = [0]\n",
    "#             user_max_idx = top_k_recs[idx, 0] \n",
    "#             recs_idxs = [user_max_idx]\n",
    "\n",
    "#             #hold the features of the recommended items\n",
    "#             recs_features = self.items_hat[:,user_max_idx]\n",
    "\n",
    "#             for r in range(1,k):\n",
    "\n",
    "#                 if r == 1:\n",
    "#                     #for the second item, just use the first item values\n",
    "#                     centroid = recs_features\n",
    "#                 else:\n",
    "#                     centroid = np.nanmean(recs_features, axis=0)\n",
    "\n",
    "#                 centroid = centroid.reshape(1, -1)\n",
    "\n",
    "#                 #set all the previously chosen item features to the centroid, so they will not be selected again\n",
    "#                 #don't want to just remove rows because it will throw off the indexing\n",
    "#                 user_item_feats[:, 0:r+1]=centroid.T\n",
    "\n",
    "#                 d = pairwise_distances(X=centroid, Y=user_item_feats.T, metric='cityblock',force_all_finite='allow_nan' )\n",
    "\n",
    "#                 most_distant = np.argmax(d)\n",
    "\n",
    "#                 distances.append(d.max())\n",
    "\n",
    "#                 most_distant_feats = user_item_feats.T[most_distant]\n",
    "\n",
    "#                 #get the index of the most distant item in the top k recs\n",
    "#                 recs_idxs.append(top_k_recs[idx, most_distant])\n",
    "#                 recs_features = np.vstack((recs_features, user_item_feats[:, most_distant]))\n",
    "#                 print (recs_idxs)\n",
    "#             rec.append(recs_idxs)\n",
    "    \n",
    "#     return rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def latent_factors_diversification(user_features, item_features, n_recs=10, top_n_limit=None):\n",
    "\n",
    "\n",
    "#     hat_ratings = np.dot(user_features, item_features.T) \n",
    "\n",
    "#     if top_n_limit:\n",
    "#         #if constraining by top n, only retain the top n ratings within each user\n",
    "#         ind=np.argpartition(hat_ratings,-top_n_limit)[:,-top_n_limit:]\n",
    "#         n_ratings = np.take(hat_ratings, ind)\n",
    "#     else:\n",
    "#         #if not constraining by top n, retail all item indices for all users. \n",
    "#         #If this is the case, in all_user_recs, recs_idxs should match original_recs_idxs\n",
    "#         ind=np.tile(np.arange(0,len(item_features)),(len(user_features),1))\n",
    "#         n_ratings = hat_ratings\n",
    "\n",
    "\n",
    "\n",
    "#     all_user_recs = dict()\n",
    "    \n",
    "#     max_idx = np.argmax(n_ratings, axis=1)\n",
    "#     top_items=item_features[max_idx]\n",
    "    \n",
    "#     all_recs = np.empty([user_features.shape[0],item_features.shape[1], n_recs])\n",
    "#     #all_recs = None\n",
    "    \n",
    "\n",
    "#     for idx, user in enumerate(user_features):\n",
    "\n",
    "#         user_item_feats = item_features[ind[idx]]\n",
    "#         user_max_idx = np.argmax(n_ratings[idx])\n",
    "\n",
    "#         #get the top rec and add that as the first item for each user\n",
    "#         user_max = max_idx[idx]\n",
    "#         recs_features = top_items[idx]\n",
    "#         recs_idxs = [max_idx[idx]]\n",
    "#         recs_preds = [n_ratings[idx][user_max]]\n",
    "#         orig_recs_idxs = [ind[idx, user_max]]\n",
    "\n",
    "\n",
    "\n",
    "#         for rec in range(1,n_recs):\n",
    "#             if rec == 1:\n",
    "#                 #for the second item, just use the first item values\n",
    "#                 centroid = recs_features\n",
    "#             else:\n",
    "#                 centroid = np.nanmean(recs_features, axis=0)\n",
    "\n",
    "#             centroid = centroid.reshape(1, -1)\n",
    "\n",
    "#             #set all the previously chosen item features to the centroid, so they will not be selected again\n",
    "#             #don't want to just remove rows because it will throw of the indexing\n",
    "#             user_item_feats[recs_idxs]=centroid\n",
    "\n",
    "#             d = pairwise_distances(X=centroid, Y=user_item_feats, metric='cityblock',force_all_finite='allow_nan' )\n",
    "#             most_distant = np.argmax(d)\n",
    "\n",
    "#             recs_idxs.append(most_distant)\n",
    "#             #get the item index from the original array of indices, not the constrained array\n",
    "#             orig_recs_idxs.append(ind[idx, most_distant])\n",
    "#             recs_preds.append(n_ratings[idx][most_distant])\n",
    "\n",
    "#             recs_features = np.vstack((recs_features, user_item_feats[most_distant]))\n",
    "\n",
    "#         all_recs[idx, :, :]=recs_features\n",
    "            \n",
    "#         all_user_recs[idx]={'user_feats': user,\n",
    "#                         'original_recs_idx':orig_recs_idxs,\n",
    "#                         'recs_idx':recs_idxs,\n",
    "#                         'recs_features':recs_features,\n",
    "#                         'recs_preds':recs_preds}\n",
    "\n",
    "        \n",
    "#     return all_recs, all_user_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trecsEnv",
   "language": "python",
   "name": "trecsenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
